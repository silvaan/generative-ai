{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e02b5453",
   "metadata": {},
   "source": [
    "# Avaliação de Modelos de Geração de Imagens\n",
    "\n",
    "A avaliação de modelos generativos, especificamente aqueles baseados em difusão, apresenta desafios únicos quando comparada a tarefas supervisionadas clássicas. Não existe uma métrica única que capture toda a complexidade da geração de imagens; portanto, é necessário analisar o desempenho sob múltiplas óticas: qualidade perceptual (fidelidade), diversidade da distribuição gerada e alinhamento semântico com o prompt de entrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b89b0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from diffusers import StableDiffusionPipeline, StableDiffusionImg2ImgPipeline\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "from torchmetrics.image.inception import InceptionScore\n",
    "from torchmetrics.multimodal.clip_score import CLIPScore\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f28c168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definição do dispositivo de processamento\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device being used: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c40ccc",
   "metadata": {},
   "source": [
    "## Geração com Stable Diffusion 1.5\n",
    "\n",
    "O Stable Diffusion 1.5 é um modelo de difusão latente (LDM) que opera no espaço comprimido de um autoencoder variacional (VAE), em vez de operar diretamente no espaço de pixel. Isso reduz drasticamente o custo computacional. Para a inferência, utilizamos a biblioteca `diffusers`, que abstrai o pipeline de denoising, o agendador (scheduler) e a decodificação via VAE. A tarefa de *text-to-image* envolve condicionar o processo de denoising reverso através de embeddings de texto gerados por um codificador CLIP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307346ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregamento do pipeline pré-treinado\n",
    "model_id = \"runwayml/stable-diffusion-v1-5\"\n",
    "pipe_txt2img = StableDiffusionPipeline.from_pretrained(model_id).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78161902",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"a photograph of an astronaut riding a horse on mars, high resolution, 8k\"\n",
    "negative_prompt = \"blurry, low quality, distorted\"\n",
    "\n",
    "image_generated = pipe_txt2img(\n",
    "    prompt=prompt,\n",
    "    negative_prompt=negative_prompt,\n",
    "    num_inference_steps=50,\n",
    "    guidance_scale=7.5\n",
    ").images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ca9e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(image_generated)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Generated Image (Text-to-Image)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551b5468",
   "metadata": {},
   "source": [
    "### Pipeline Image-to-Image\n",
    "\n",
    "A tarefa de *image-to-image* difere da geração pura pois o processo de difusão não inicia a partir de um ruído gaussiano puro $\\mathcal{N}(0, I)$. Em vez disso, a imagem de entrada é codificada para o espaço latente e ruído é adicionado a ela até um certo passo de tempo $t$, controlado por um hiperparâmetro frequentemente denominado *strength*. O processo de denoising reverso então guia a geração em direção ao novo prompt, mantendo a estrutura semântica e composicional da imagem original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49996384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reutilizamos os componentes do pipe anterior para economizar memória\n",
    "pipe_img2img = StableDiffusionImg2ImgPipeline.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16,\n",
    "    safety_checker=None\n",
    ")\n",
    "pipe_img2img = pipe_img2img.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49308a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_i2i = \"a oil painting of an astronaut riding a horse on mars, van gogh style\"\n",
    "\n",
    "# Geração a partir da imagem anterior\n",
    "image_i2i = pipe_img2img(\n",
    "    prompt=prompt_i2i,\n",
    "    image=image_generated,\n",
    "    strength=0.75,\n",
    "    guidance_scale=8.0\n",
    ").images[0]\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(image_i2i)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Generated Image (Image-to-Image)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b34f122",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/img2img/sketch-mountains-input.jpg\"\n",
    "response = requests.get(url)\n",
    "init_image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "init_image = init_image.resize((768, 512)) # Redimensionando para tamanho padrão\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(init_image)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Initial Image\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32f9c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_i2i = \"a fantasy landscape, realistic mountains, 8k, photorealistic, lord of the rings style\"\n",
    "\n",
    "image_i2i = pipe_img2img(\n",
    "    prompt=prompt_i2i,\n",
    "    image=init_image,\n",
    "    strength=0.75,\n",
    "    guidance_scale=8.0\n",
    ").images[0]\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(image_i2i)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Generated Image (Image-to-Image)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbd8962",
   "metadata": {},
   "source": [
    "## Fréchet Inception Distance (FID)\n",
    "\n",
    "A Fréchet Inception Distance (FID) é a métrica padrão-ouro para avaliar o realismo e a diversidade de imagens geradas. Ela calcula a distância de Wasserstein-2 entre duas distribuições multivariadas de Gaussianas ajustadas às características extraídas de uma rede Inception-v3 pré-treinada (geralmente na camada de *pooling* anterior à classificação).\n",
    "\n",
    "Sejam $(\\mu_r, \\Sigma_r)$ a média e a matriz de covariância das características das imagens reais, e $(\\mu_g, \\Sigma_g)$ as estatísticas das imagens geradas. O FID é definido como:\n",
    "\n",
    "$$\n",
    "FID = ||\\mu_r - \\mu_g||^2 + Tr(\\Sigma_r + \\Sigma_g - 2(\\Sigma_r \\Sigma_g)^{1/2})\n",
    "$$\n",
    "\n",
    "Um valor de FID menor indica que a distribuição das imagens geradas é mais próxima da distribuição das imagens reais, implicando melhor qualidade visual e diversidade similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdfa857",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "to_uint8_299 = transforms.Compose([\n",
    "    transforms.Resize((299, 299)),\n",
    "    transforms.ToTensor(),\n",
    "    lambda x: (x * 255).to(torch.uint8)\n",
    "])\n",
    "\n",
    "prompts_batch = [\n",
    "    \"a photograph of an astronaut riding a horse on mars, high resolution\",\n",
    "    \"a cyberpunk city at night, ultra detailed, 8k\",\n",
    "    \"a portrait of a medieval knight, cinematic lighting\",\n",
    "    \"a fantasy landscape with dragons and castles, 8k\",\n",
    "    \"a close up photo of a cat wearing sunglasses, 4k\",\n",
    "]\n",
    "\n",
    "negative_prompts_batch = [\"blurry, low quality, distorted\"] * len(prompts_batch)\n",
    "\n",
    "with torch.no_grad():\n",
    "    out = pipe_txt2img(\n",
    "        prompt=prompts_batch,\n",
    "        negative_prompt=negative_prompts_batch,\n",
    "        num_inference_steps=30,\n",
    "        guidance_scale=7.5\n",
    "    )\n",
    "\n",
    "generated_images_uint8 = torch.stack(\n",
    "    [to_uint8_299(img) for img in out.images]\n",
    ").to(device)\n",
    "\n",
    "print(\"Shape das imagens geradas:\", generated_images_uint8.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87f22a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, len(prompts_batch), figsize=(3 * len(prompts_batch), 3))\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.imshow(generated_images_uint8[i].cpu().permute(1, 2, 0).numpy())\n",
    "    ax.set_title(f\"Sample {i}\")\n",
    "    ax.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afb4f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset real para servir de referência nas métricas (exemplo com CIFAR-10)\n",
    "real_transform = transforms.Compose([\n",
    "    transforms.Resize((299, 299)),\n",
    "    transforms.ToTensor(),\n",
    "    lambda x: (x * 255).to(torch.uint8)\n",
    "])\n",
    "\n",
    "real_dataset = datasets.CIFAR10(\n",
    "    root=\"data/data_cifar\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=real_transform\n",
    ")\n",
    "\n",
    "real_loader = DataLoader(real_dataset, batch_size=len(prompts_batch), shuffle=True)\n",
    "real_images_uint8, _ = next(iter(real_loader))\n",
    "real_images_uint8 = real_images_uint8.to(device)\n",
    "\n",
    "print(\"Shape das imagens reais para as métricas:\", real_images_uint8.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a9b858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicialização da métrica\n",
    "fid = FrechetInceptionDistance(feature=64).to(device)\n",
    "\n",
    "# Atualiza com imagens reais\n",
    "fid.update(real_images_uint8, real=True)\n",
    "\n",
    "# Atualiza com imagens geradas pelo Stable Diffusion\n",
    "fid.update(generated_images_uint8, real=False)\n",
    "\n",
    "# Computação do score\n",
    "fid_score = fid.compute()\n",
    "print(f\"FID Score: {fid_score.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715b497a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fid.reset()\n",
    "\n",
    "# Simulação de dados para exemplo\n",
    "fake_images = torch.randint(0, 255, (5, 3, 299, 299), dtype=torch.uint8).to(device)\n",
    "\n",
    "fid.update(real_images_uint8, real=True)\n",
    "fid.update(fake_images, real=False)\n",
    "\n",
    "fid_score = fid.compute()\n",
    "print(f\"FID Score: {fid_score.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c076339c",
   "metadata": {},
   "source": [
    "## Inception Score (IS)\n",
    "\n",
    "O Inception Score (IS) avalia a qualidade das imagens focando em dois aspectos: nitidez (se a imagem é claramente identificável como uma classe específica) e diversidade (se o modelo gera uma variedade de classes). Ele utiliza a divergência KL entre a distribuição condicional de classes $p(y|x)$ e a distribuição marginal $p(y)$.\n",
    "\n",
    "Matematicamente, para um conjunto de imagens geradas $x$, o IS é calculado como:\n",
    "\n",
    "$$\n",
    "IS = \\exp\\left( \\mathbb{E}_{x \\sim p_g} [ D_{KL}(p(y|x) || p(y)) ] \\right)\n",
    "$$\n",
    "\n",
    "Onde $p(y|x)$ é a probabilidade da classe dada a imagem (calculada pela Inception-v3) e $p(y) \\approx \\frac{1}{N} \\sum_{i=1}^N p(y|x_i)$ é a distribuição marginal das classes. Valores maiores indicam melhor desempenho."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea687c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicialização da métrica\n",
    "inception_score = InceptionScore(feature=\"logits_unbiased\").to(device)\n",
    "\n",
    "# Aqui usamos o mesmo batch generated_images_uint8\n",
    "inception_score.update(generated_images_uint8)\n",
    "\n",
    "is_mean, is_std = inception_score.compute()\n",
    "\n",
    "print(f\"Inception Score: Mean = {is_mean.item()}, Std = {is_std.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d059ed83",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_generated_batch = torch.randint(0, 255, (32, 3, 299, 299), dtype=torch.uint8).to(device)\n",
    "\n",
    "# Atualização e cálculo\n",
    "inception_score.update(imgs_generated_batch)\n",
    "is_mean, is_std = inception_score.compute()\n",
    "\n",
    "print(f\"Inception Score: Mean = {is_mean.item()}, Std = {is_std.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a793590",
   "metadata": {},
   "source": [
    "## CLIP Score\n",
    "\n",
    "Diferente das métricas anteriores que avaliam a distribuição da imagem, o CLIP Score mede o alinhamento semântico entre o prompt de texto e a imagem gerada. Ele utiliza o modelo CLIP (Contrastive Language-Image Pre-training) que projeta texto e imagem em um espaço latente compartilhado.\n",
    "\n",
    "O CLIP Score é definido como a similaridade de cosseno entre o embedding da imagem $E_I$ e o embedding do texto $E_T$, frequentemente escalado por um fator:\n",
    "\n",
    "$$\n",
    "CLIP(I, T) = \\max(100 \\cdot \\cos(E_I, E_T), 0)\n",
    "$$\n",
    "\n",
    "Essa métrica é crucial para verificar se o modelo está obedecendo às condicionantes textuais fornecidas pelo usuário."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012c0061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicialização da métrica CLIP\n",
    "metric_clip = CLIPScore(model_name_or_path=\"openai/clip-vit-base-patch16\").to(device)\n",
    "\n",
    "# Normalizar para [0, 1] como float\n",
    "images_for_clip = (generated_images_uint8.float() / 255.0).to(device)\n",
    "\n",
    "clip_score = metric_clip(images_for_clip, prompts_batch)\n",
    "\n",
    "print(f\"CLIP Score: {clip_score.detach().item()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
