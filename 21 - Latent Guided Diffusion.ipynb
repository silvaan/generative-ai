{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c536e5e5",
   "metadata": {},
   "source": [
    "# Classifier Guidance em Latent Diffusion com MNIST\n",
    "\n",
    "Neste notebook vamos implementar um pipeline didático de Latent Diffusion com MNIST, incluindo:\n",
    "\n",
    "- Um autoencoder simples que define o espaço latente.\n",
    "- Um modelo de difusão treinado no espaço latente.\n",
    "- Um classificador que recebe latentes ruidosos e prevê o dígito.\n",
    "- Amostragem guiada por classe usando classifier guidance, modificando os passos de denoising via gradiente do classificador.\n",
    "\n",
    "O objetivo é entender a ideia de classifier guidance no contexto de difusão latente, mais do que obter imagens de altíssima qualidade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c008e6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d54355f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d821c675",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()  # [0,1]\n",
    "])\n",
    "\n",
    "train_ds = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
    "test_ds  = datasets.MNIST(root=\"./data\", train=False, download=True, transform=transform)\n",
    "\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "test_loader  = DataLoader(test_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Visualizar alguns exemplos\n",
    "images, labels = next(iter(train_loader))\n",
    "plt.figure(figsize=(10, 4))\n",
    "for i in range(8):\n",
    "    plt.subplot(1, 8, i+1)\n",
    "    plt.imshow(images[i, 0].numpy(), cmap=\"gray\")\n",
    "    plt.title(int(labels[i]))\n",
    "    plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d44c92d",
   "metadata": {},
   "source": [
    "## Autoencoder Variacional (VAE)\n",
    "\n",
    "O primeiro componente do nosso sistema é o VAE. O objetivo é comprimir a imagem de entrada $x \\in \\mathbb{R}^{784}$ em um vetor latente $z \\in \\mathbb{R}^{d}$, onde $d \\ll 784$. O VAE aprende uma distribuição probabilística $q_\\phi(z|x)$ (codificador) e $p_\\theta(x|z)$ (decodificador).\n",
    "\n",
    "Para permitir o treinamento via backpropagation, utilizamos o **truque de reparametrização**. Em vez de amostrar $z$ diretamente da distribuição estocástica, definimos:\n",
    "\n",
    "$$\n",
    "z = \\mu + \\sigma \\odot \\epsilon, \\quad \\text{onde } \\epsilon \\sim \\mathcal{N}(0, I)\n",
    "$$\n",
    "\n",
    "Isso permite que o gradiente flua deterministicamente através de $\\mu$ e $\\sigma$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ac5b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, latent_dim=32):\n",
    "        super().__init__()\n",
    "\n",
    "        # encoder\n",
    "        self.enc = nn.Sequential(\n",
    "            nn.Linear(784, 400),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc_mu = nn.Linear(400, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(400, latent_dim)\n",
    "\n",
    "        # decoder\n",
    "        self.dec = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 400),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(400, 784),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.enc(x)\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparam(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        x_hat = self.dec(z)\n",
    "        return x_hat.view(-1, 1, 28, 28)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparam(mu, logvar)\n",
    "        x_hat = self.decode(z)\n",
    "        return x_hat, mu, logvar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3253342d",
   "metadata": {},
   "source": [
    "### Função de Perda: ELBO\n",
    "\n",
    "A função de perda do VAE é derivada do *Evidence Lower Bound* (ELBO). Ela maximiza a probabilidade dos dados enquanto minimiza a divergência entre a distribuição latente aproximada e uma priori (geralmente Gaussiana unitária). A perda é composta por dois termos:\n",
    "\n",
    "1.  **Erro de Reconstrução:** Mede a fidelidade da imagem decodificada $\\hat{x}$ em relação a $x$.\n",
    "2.  **Divergência KL:** Regulariza o espaço latente para que $q_\\phi(z|x)$ se aproxime de $\\mathcal{N}(0, I)$.\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{VAE} = \\mathbb{E}_{q}[\\log p(x|z)] - D_{KL}(q(z|x) || p(z))\n",
    "$$\n",
    "\n",
    "Na prática, minimizamos o negativo do ELBO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efc5a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss(x, x_hat, mu, logvar):\n",
    "    recon = F.binary_cross_entropy(x_hat, x, reduction=\"sum\")\n",
    "    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return recon + kl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c369368",
   "metadata": {},
   "source": [
    "### Treinamento do VAE\n",
    "\n",
    "Treinaremos o VAE por poucas épocas, o suficiente para obtermos uma representação latente coerente. Note que aplicamos um peso $\\beta$ (no código ajustado para 0.1) ao termo KL para evitar o *colapso posterior*, onde o codificador ignora a entrada e produz apenas a priori, o que prejudicaria a capacidade de reconstrução."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a59b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 16\n",
    "vae = VAE(latent_dim).to(device)\n",
    "optimizer_vae = torch.optim.Adam(vae.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a355ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_epochs = 20\n",
    "\n",
    "vae.train()\n",
    "for epoch in range(vae_epochs):\n",
    "    total_loss = 0\n",
    "\n",
    "    for x, _ in train_loader:\n",
    "        x = x.to(device)\n",
    "\n",
    "        optimizer_vae.zero_grad()\n",
    "\n",
    "        # forward\n",
    "        recon_x, mu, logvar = vae(x)\n",
    "\n",
    "        # perda de reconstrução (MSE)\n",
    "        loss_recon = F.mse_loss(recon_x, x, reduction=\"sum\")\n",
    "\n",
    "        # KL Divergence\n",
    "        loss_kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "        # Total (com peso KL reduzido para estabilidade)\n",
    "        loss = loss_recon + 0.1 * loss_kl\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer_vae.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"[VAE] Epoch {epoch+1}/{vae_epochs} Loss: {total_loss / len(train_loader.dataset):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8d71c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.eval()\n",
    "x, _ = next(iter(test_loader))\n",
    "x = x.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    x_hat, _, _ = vae(x)\n",
    "\n",
    "x = x.cpu()\n",
    "x_hat = x_hat.cpu()\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "for i in range(8):\n",
    "    plt.subplot(2,8,i+1)\n",
    "    plt.imshow(x[i,0], cmap=\"gray\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(2,8,8+i+1)\n",
    "    plt.imshow(x_hat[i,0], cmap=\"gray\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.suptitle(\"VAE - Original (linha 1) vs Reconstrução (linha 2)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877d6cf6",
   "metadata": {},
   "source": [
    "## Difusão Latente\n",
    "\n",
    "Esta é a etapa que caracteriza o *Latent Diffusion*. Em vez de treinarmos o modelo de difusão nas imagens brutas, nós \"congelamos\" o VAE e codificamos todo o dataset MNIST para o espaço latente $z$.\n",
    "\n",
    "A partir de agora, nosso dataset de treinamento consiste em pares $(z, y)$, onde $z$ são os vetores latentes amostrados e $y$ são os rótulos das classes. Isso reduz a dimensionalidade do problema e foca o modelo de difusão na aprendizagem da semântica estrutural dos dados comprimidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5d8564",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_dataset(loader, vae):\n",
    "    vae.eval()\n",
    "    zs, ys = [], []\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            mu, logvar = vae.encode(x.view(x.size(0), -1))\n",
    "            z = vae.reparam(mu, logvar)\n",
    "            zs.append(z.cpu())\n",
    "            ys.append(y)\n",
    "    return torch.cat(zs), torch.cat(ys)\n",
    "\n",
    "train_z, train_y = encode_dataset(train_loader, vae)\n",
    "test_z, test_y = encode_dataset(test_loader, vae)\n",
    "\n",
    "train_latent = TensorDataset(train_z, train_y)\n",
    "test_latent  = TensorDataset(test_z, test_y)\n",
    "\n",
    "train_latent_loader = DataLoader(train_latent, batch_size=batch_size, shuffle=True)\n",
    "test_latent_loader  = DataLoader(test_latent, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61176a8",
   "metadata": {},
   "source": [
    "### Processo Forward\n",
    "\n",
    "Definimos o processo de difusão *forward* como uma cadeia de Markov fixa que adiciona ruído Gaussiano gradualmente aos dados. Seguindo a formulação DDPM (Ho et al., 2020), definimos uma schedule de variância $\\beta_t$.\n",
    "\n",
    "A propriedade fundamental das gaussianas nos permite amostrar $z_t$ diretamente de $z_0$ sem iterar pelos passos intermediários:\n",
    "\n",
    "$$\n",
    "q(z_t | z_0) = \\mathcal{N}(z_t; \\sqrt{\\bar{\\alpha}_t} z_0, (1 - \\bar{\\alpha}_t)I)\n",
    "$$\n",
    "\n",
    "Onde $\\alpha_t = 1 - \\beta_t$ e $\\bar{\\alpha}_t = \\prod_{s=1}^t \\alpha_s$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b976eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 20\n",
    "betas = torch.linspace(1e-4, 0.02, T).to(device)\n",
    "alphas = 1 - betas\n",
    "alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
    "\n",
    "sqrt_ac = torch.sqrt(alphas_cumprod)\n",
    "sqrt_omac = torch.sqrt(1 - alphas_cumprod)\n",
    "\n",
    "def forward_diffusion(z0, t):\n",
    "    noise = torch.randn_like(z0)\n",
    "    zt = sqrt_ac[t].unsqueeze(1) * z0 + sqrt_omac[t].unsqueeze(1) * noise\n",
    "    return zt, noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c1628b",
   "metadata": {},
   "source": [
    "### Modelo de Difusão (Denoising Network)\n",
    "\n",
    "Nossa rede neural, $\\epsilon_\\theta(z_t, t)$, é um Perceptron Multicamadas (MLP) condicionado pelo tempo. Seu objetivo não é prever $z_0$ diretamente, mas sim estimar o ruído $\\epsilon$ que foi adicionado à imagem no tempo $t$.\n",
    "\n",
    "A entrada da rede é a concatenação do latente ruidoso $z_t$ e o embedding de tempo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f130044",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidalTimeEmbedding(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        half_dim = emb_dim // 2\n",
    "        inv_freq = torch.exp(\n",
    "            -torch.arange(0, half_dim, dtype=torch.float32) * np.log(10000.0) / half_dim\n",
    "        )\n",
    "        self.register_buffer(\"inv_freq\", inv_freq)\n",
    "\n",
    "    def forward(self, t):\n",
    "        # t: (B,)\n",
    "        t = t.float().unsqueeze(1)  # (B,1)\n",
    "        freqs = t * self.inv_freq.unsqueeze(0)  # (B, half_dim)\n",
    "        emb = torch.cat([torch.sin(freqs), torch.cos(freqs)], dim=-1)\n",
    "        if self.emb_dim % 2 == 1:\n",
    "            emb = torch.cat([emb, torch.zeros_like(emb[:, :1])], dim=-1)\n",
    "        return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21861e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentDiffusionModel(nn.Module):\n",
    "    def __init__(self, latent_dim, time_emb_dim=64, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.time_mlp = SinusoidalTimeEmbedding(time_emb_dim)\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(latent_dim + time_emb_dim, hidden_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_dim, latent_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, zt, t):\n",
    "        # zt: (B, latent_dim), t: (B,)\n",
    "        t_emb = self.time_mlp(t)\n",
    "        x = torch.cat([zt, t_emb], dim=-1)\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9610077f",
   "metadata": {},
   "source": [
    "### Treinamento do Modelo de Difusão\n",
    "\n",
    "O objetivo de treinamento é minimizar o erro quadrático médio (MSE) entre o ruído real adicionado e o ruído predito pelo modelo. A função de perda simplificada é dada por:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{simple}} = \\mathbb{E}_{z_0, \\epsilon, t} \\left[ \\| \\epsilon - \\epsilon_\\theta(z_t, t) \\|^2 \\right]\n",
    "$$\n",
    "\n",
    "Amostramos aleatoriamente passos de tempo $t$ e vetores de ruído $\\epsilon$ para cada batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f3206d",
   "metadata": {},
   "outputs": [],
   "source": [
    "diffusion_model = LatentDiffusionModel(latent_dim).to(device)\n",
    "diff_optimizer = torch.optim.Adam(diffusion_model.parameters(), lr=2e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8502eec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_timesteps(b, T):\n",
    "    return torch.randint(0, T, (b,), device=device)\n",
    "\n",
    "n_epochs_diff = 20\n",
    "\n",
    "for epoch in range(n_epochs_diff):\n",
    "    diffusion_model.train()\n",
    "    running_loss = 0.0\n",
    "    for z0, _ in train_latent_loader:\n",
    "        z0 = z0.to(device)\n",
    "        b = z0.size(0)\n",
    "        t = sample_timesteps(b, T)\n",
    "        zt, noise = forward_diffusion(z0, t)\n",
    "\n",
    "        noise_pred = diffusion_model(zt, t)\n",
    "        loss = F.mse_loss(noise_pred, noise)\n",
    "\n",
    "        diff_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        diff_optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * b\n",
    "\n",
    "    epoch_loss = running_loss / len(train_latent_loader.dataset)\n",
    "    print(f\"[Diff] Epoch {epoch+1}/{n_epochs_diff} - Loss: {epoch_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cafcea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Amostra um batch\n",
    "z0, _ = next(iter(train_latent_loader))\n",
    "z0 = z0[:8].to(device)\n",
    "\n",
    "# Visualiza forward\n",
    "fig, axes = plt.subplots(3, 8, figsize=(10,4))\n",
    "\n",
    "timesteps = [0, T//2, T-1]\n",
    "for row, t in enumerate(timesteps):\n",
    "    zt, _ = forward_diffusion(z0, torch.full((8,), t, device=device))\n",
    "    imgs = vae.decode(zt).cpu().detach()\n",
    "\n",
    "    for col in range(8):\n",
    "        axes[row, col].imshow(imgs[col,0], cmap=\"gray\")\n",
    "        axes[row,col].axis(\"off\")\n",
    "    axes[row,0].set_ylabel(f\"t={t}\")\n",
    "\n",
    "plt.suptitle(\"Forward diffusion no espaço latente\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed55fe58",
   "metadata": {},
   "source": [
    "## Classificador Latente\n",
    "\n",
    "Para aplicar o **Classifier Guidance**, precisamos de um classificador $p_\\phi(y | z_t, t)$. É crucial que este classificador seja treinado em representações latentes ruidosas $z_t$, pois durante a geração ele precisará fornecer gradientes para guiar a difusão a partir de estados altamente ruidosos.\n",
    "\n",
    "O classificador recebe como entrada $z_t$ e o tempo $t$, e prevê o dígito $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff9e04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentClassifier(nn.Module):\n",
    "    def __init__(self, latent_dim, time_dim=64, hidden=256):\n",
    "        super().__init__()\n",
    "        self.time = SinusoidalTimeEmbedding(time_dim)\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(latent_dim + time_dim, hidden),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden, hidden),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, zt, t):\n",
    "        t_emb = self.time(t)\n",
    "        return self.net(torch.cat([zt, t_emb], dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1aff0d1",
   "metadata": {},
   "source": [
    "### Treinamento do Classificador\n",
    "\n",
    "O treinamento simula o processo de difusão: para cada batch, amostramos um tempo $t$ e adicionamos ruído ao latente $z_0$ para obter $z_t$. O classificador é treinado com Cross Entropy convencional para prever a classe correta a partir dessa entrada ruidosa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d356dfe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LatentClassifier(latent_dim).to(device)\n",
    "opt_clf = torch.optim.Adam(clf.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d0f0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs_clf = 10\n",
    "\n",
    "for e in range(n_epochs_clf):\n",
    "    clf.train()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "\n",
    "    for z0, y in train_latent_loader:\n",
    "        z0, y = z0.to(device), y.to(device)\n",
    "        b = z0.size(0)\n",
    "\n",
    "        t = torch.randint(0, T, (b,), device=device)\n",
    "        zt, _ = forward_diffusion(z0, t)\n",
    "\n",
    "        logits = clf(zt, t)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "\n",
    "        opt_clf.zero_grad()\n",
    "        loss.backward()\n",
    "        opt_clf.step()\n",
    "\n",
    "        total += b\n",
    "        correct += (logits.argmax(1) == y).sum().item()\n",
    "\n",
    "    print(f\"[CLF] Epoch {e+1} Acc: {correct/total:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46ffd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "clf.eval()\n",
    "all_preds = []\n",
    "all_true = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for z0, y in test_latent_loader:\n",
    "        z0 = z0.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        t = torch.randint(0, T, (z0.size(0),), device=device)\n",
    "        zt, _ = forward_diffusion(z0, t)\n",
    "\n",
    "        preds = clf(zt, t).argmax(1)\n",
    "        all_preds.append(preds.cpu())\n",
    "        all_true.append(y.cpu())\n",
    "\n",
    "all_preds = torch.cat(all_preds)\n",
    "all_true = torch.cat(all_true)\n",
    "\n",
    "cm = confusion_matrix(all_true, all_preds)\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(cm, annot=False, cmap=\"Blues\")\n",
    "plt.title(\"Matriz de confusão do classificador no espaço latente\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2ea899",
   "metadata": {},
   "source": [
    "## Amostragem com Classifier Guidance\n",
    "\n",
    "Esta é a parte central da técnica. No processo de amostragem reverso padrão, estimamos a média da transição $p_\\theta(z_{t-1}|z_t)$ como $\\mu_\\theta(z_t, t)$. Com guidance, perturbamos essa média utilizando o gradiente do classificador:\n",
    "\n",
    "$$\n",
    "\\hat{\\mu}_\\theta(z_t, t) = \\mu_\\theta(z_t, t) + s \\cdot \\Sigma_\\theta(z_t, t) \\nabla_{z_t} \\log p_\\phi(y|z_t, t)\n",
    "$$\n",
    "\n",
    "Onde $s$ é a escala de guidance (*guidance scale*).\n",
    "- Se $s > 1$, forçamos o modelo a gerar amostras que o classificador reconhece com alta confiança (maior fidelidade, menor diversidade).\n",
    "- O gradiente $\\nabla_{z_t}$ indica a direção no espaço latente que maximiza a probabilidade da classe alvo $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3664bd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_sample_step(diff_model, clf, zt, t, y, guidance=3.0):\n",
    "    b = zt.size(0)\n",
    "    t_batch = torch.full((b,), t, device=device, dtype=torch.long)\n",
    "\n",
    "    # epsθ\n",
    "    with torch.no_grad():\n",
    "        eps_theta = diff_model(zt, t_batch)\n",
    "\n",
    "    # grad log p(y|z_t)\n",
    "    zt_req = zt.clone().detach().requires_grad_(True)\n",
    "    logits = clf(zt_req, t_batch)\n",
    "    log_probs = F.log_softmax(logits, dim=1)\n",
    "    selected = log_probs[torch.arange(b), y]\n",
    "\n",
    "    grad = torch.autograd.grad(selected.sum(), zt_req)[0]\n",
    "\n",
    "    eps_guided = eps_theta - guidance * grad\n",
    "\n",
    "    beta_t = betas[t]\n",
    "    alpha_t = alphas[t]\n",
    "    alpha_bar_t = alphas_cumprod[t]\n",
    "    sqrt_om = torch.sqrt(1 - alpha_bar_t)\n",
    "\n",
    "    mean = (1/torch.sqrt(alpha_t)) * (zt - (beta_t / sqrt_om) * eps_guided)\n",
    "\n",
    "    if t > 0:\n",
    "        return mean + torch.sqrt(beta_t) * torch.randn_like(zt)\n",
    "    return mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82d0568",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_guided(diff_model, clf, vae, num=16, digit=3, guidance=3.0):\n",
    "    z = torch.randn(num, latent_dim, device=device)\n",
    "    y = torch.full((num,), digit, device=device)\n",
    "\n",
    "    for t in reversed(range(T)):\n",
    "        z = p_sample_step(diff_model, clf, z, t, y, guidance)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        x = vae.decode(z).cpu()\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470ffdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(10, 8, figsize=(8,10))\n",
    "\n",
    "for digit in range(10):\n",
    "    samples = sample_guided(diffusion_model, clf, vae, num=8, digit=digit, guidance=7.0)\n",
    "    for i in range(8):\n",
    "        axes[digit, i].imshow(samples[i,0], cmap=\"gray\")\n",
    "        axes[digit, i].axis(\"off\")\n",
    "\n",
    "plt.suptitle(\"Latent Diffusion + Classifier Guidance (por classe)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443282f0",
   "metadata": {},
   "source": [
    "## Exercícios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743b850b",
   "metadata": {},
   "source": [
    "### Exercício 1\n",
    "\n",
    "Altere a dimensão latente do VAE e o número de passos no modelo de difusão. Qual combinação foi melhor para imagens mais realistas geradas?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3709542",
   "metadata": {},
   "source": [
    "### Exercício 2\n",
    "\n",
    "Experimente variar o fator de guidance na geração guiada. O que você observa? Qual o valor gera imagens mais realistas?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
