{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "768e7e34",
   "metadata": {},
   "source": [
    "# Deep Convolutional Generative Adversarial Networks (DCGAN)\n",
    "\n",
    "Neste notebook, exploraremos a arquitetura DCGAN, uma das primeiras e mais influentes arquiteturas de redes adversariais generativas que utiliza majoritariamente camadas convolucionais. As DCGANs trouxeram um conjunto de diretrizes arquitetônicas que não apenas estabilizaram o treinamento de GANs, mas também permitiram a geração de imagens com maior qualidade e resolução. Assumimos que os conceitos fundamentais de GANs, como o jogo minimax entre um Gerador e um Discriminador, já foram compreendidos. Aqui, focaremos nos detalhes técnicos e na implementação que tornam a DCGAN eficaz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f633aea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.utils as vutils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3362c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6cb2f5",
   "metadata": {},
   "source": [
    "### Preparação dos Dados (MNIST)\n",
    "\n",
    "Para treinar nossa DCGAN, utilizaremos o dataset MNIST. As imagens serão redimensionadas para 32x32 para facilitar uma arquitetura com múltiplas camadas de convolução transposta que dobram a dimensão espacial. Além disso, normalizaremos os pixels das imagens para o intervalo `[-1, 1]`, que corresponde à faixa da função de ativação `Tanh` na camada de saída do nosso Gerador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b9992f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "image_size = 32\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(image_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5]) # Normaliza para [-1, 1]\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root=\"./data\", train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.MNIST(root=\"./data\", train=False, transform=transform, download=True)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Número de batches de treino: {len(train_dataloader)}\")\n",
    "print(f\"Número de batches de teste: {len(test_dataloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec2da6d",
   "metadata": {},
   "source": [
    "### O Gerador\n",
    "\n",
    "O Gerador ($G$) tem a função de mapear um vetor de ruído do espaço latente ($z \\in \\mathbb{R}^{100}$) para o espaço de dados ($G(z) \\in \\mathbb{R}^{32 \\times 32}$). Ele realiza essa tarefa através de uma série de camadas de convolução transposta (`ConvTranspose2d`), que progressivamente aumentam a dimensão espacial dos mapas de características, enquanto reduzem sua profundidade (número de canais). A Batch Normalization é aplicada após cada convolução transposta para estabilizar o fluxo de gradientes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23eb7f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim, img_channels, features_g):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            # Entrada: Vetor de ruído (latent_dim)\n",
    "            # Saída: features_g*8 x 4 x 4\n",
    "            nn.ConvTranspose2d(latent_dim, features_g * 8, kernel_size=4, stride=1, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(features_g * 8),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            # Saída: features_g*4 x 8 x 8\n",
    "            nn.ConvTranspose2d(features_g * 8, features_g * 4, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(features_g * 4),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            # Saída: features_g*2 x 16 x 16\n",
    "            nn.ConvTranspose2d(features_g * 4, features_g * 2, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(features_g * 2),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            # Saída: img_channels x 32 x 32\n",
    "            nn.ConvTranspose2d(features_g * 2, img_channels, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.Tanh() # Normaliza a saída para [-1, 1]\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.net(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9198bf",
   "metadata": {},
   "source": [
    "### O Discriminador\n",
    "\n",
    "O Discriminador ($D$) é uma rede neural convolucional padrão que atua como um classificador binário. Sua tarefa é receber uma imagem (seja ela real do dataset ou falsa, gerada por $G$) e produzir um escalar que representa a probabilidade da imagem ser real. A arquitetura espelha a do Gerador, utilizando convoluções com `stride=2` para reduzir a dimensão espacial, `LeakyReLU` como função de ativação para permitir o fluxo de gradientes mesmo para entradas negativas, e `Batch Normalization`. A camada final utiliza a função `Sigmoid` para mapear a saída para o intervalo $[0, 1]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f979320",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, img_channels, features_d):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            # Entrada: img_channels x 32 x 32\n",
    "            # Saída: features_d x 16 x 16\n",
    "            nn.Conv2d(img_channels, features_d, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # Saída: features_d*2 x 8 x 8\n",
    "            nn.Conv2d(features_d, features_d * 2, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(features_d * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # Saída: features_d*4 x 4 x 4\n",
    "            nn.Conv2d(features_d * 2, features_d * 4, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(features_d * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # Saída: 1 x 1 x 1 (probabilidade)\n",
    "            nn.Conv2d(features_d * 4, 1, kernel_size=4, stride=1, padding=0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.net(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9535e216",
   "metadata": {},
   "source": [
    "### Inicialização de Pesos e Instanciação\n",
    "\n",
    "O artigo da DCGAN sugere que os pesos das camadas `Conv` e `ConvTranspose` devem ser inicializados a partir de uma distribuição Normal com média 0 e desvio padrão 0.02. Esta prática ajuda a prevenir que os gradientes saturem ou desapareçam no início do treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f1f1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "# Instanciação dos modelos\n",
    "features_g = 16\n",
    "features_d = 16\n",
    "latent_dim = 100\n",
    "img_channels = 1\n",
    "\n",
    "generator = Generator(latent_dim, img_channels, features_g).to(device)\n",
    "generator.apply(weights_init)\n",
    "\n",
    "discriminator = Discriminator(img_channels, features_d).to(device)\n",
    "discriminator.apply(weights_init)\n",
    "\n",
    "print(\"Generator Architecture:\\n\", generator)\n",
    "print(\"\\nDiscriminator Architecture:\\n\", discriminator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc5798e",
   "metadata": {},
   "source": [
    "### Função de Perda e Otimizadores\n",
    "\n",
    "O treinamento de GANs envolve a otimização de uma função de perda minimax, que para o caso discreto é a Entropia Cruzada Binária (`Binary Cross-Entropy`).\n",
    "\n",
    "A função objetivo do Discriminador é maximizar a probabilidade de classificar corretamente as imagens reais e falsas. A do Gerador é minimizar a probabilidade do Discriminador classificar suas saídas como falsas. A função de perda é:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(D, G) = \\mathbb{E}_{x \\sim p_{data}(x)}[\\log D(x)] + \\mathbb{E}_{z \\sim p_{z}(z)}[\\log(1 - D(G(z)))]\n",
    "$$\n",
    "\n",
    "Utilizaremos o otimizador Adam separadamente para o Gerador e para o Discriminador, conforme recomendado no artigo, com uma taxa de aprendizado de 0.0002 e parâmetros de beta1=0.5 para maior estabilidade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764bac48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Otimizadores e Função de Perda\n",
    "lr = 0.0002\n",
    "beta1 = 0.5\n",
    "\n",
    "optimizer_g = Adam(generator.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "optimizer_d = Adam(discriminator.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Vetor de ruído fixo para visualização da evolução do gerador\n",
    "fixed_noise = torch.randn(64, latent_dim, 1, 1, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c589b02",
   "metadata": {},
   "source": [
    "### O Loop de Treinamento\n",
    "\n",
    "O treinamento é um processo iterativo onde, a cada passo, realizamos duas atualizações: uma para o Discriminador e outra para o Gerador.\n",
    "\n",
    "1.  **Treinamento do Discriminador**:\n",
    "    * Alimentamos o Discriminador com um batch de imagens reais do dataset. Calculamos a perda ($D(x)$) em relação a labels \"reais\" (valor 1).\n",
    "    * Geramos um batch de imagens falsas usando o Gerador ($G(z)$). Alimentamos o Discriminador com essas imagens. Calculamos a perda ($D(G(z))$) em relação a labels \"falsas\" (valor 0).\n",
    "    * Somamos as duas perdas e atualizamos os pesos do Discriminador via backpropagation.\n",
    "\n",
    "2.  **Treinamento do Gerador**:\n",
    "    * Geramos um novo batch de imagens falsas ($G(z)$).\n",
    "    * Passamos essas imagens pelo Discriminador.\n",
    "    * Calculamos a perda do Gerador baseando-se na saída do Discriminador, mas desta vez, em relação a labels \"reais\" (valor 1). O objetivo do Gerador é \"enganar\" o Discriminador, fazendo-o classificar as imagens falsas como reais.\n",
    "    * Atualizamos os pesos do Gerador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db38f119",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "plot_interval = 1\n",
    "g_losses, d_losses = [], []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    generator.train()\n",
    "    discriminator.train()\n",
    "    g_loss, d_loss = 0, 0\n",
    "\n",
    "    for real_imgs, _ in train_dataloader:\n",
    "        real_imgs = real_imgs.to(device)\n",
    "        N = real_imgs.size(0)\n",
    "        valid = torch.ones(N, device=device)\n",
    "        fake = torch.zeros(N, device=device)\n",
    "\n",
    "        # --- Discriminador ---\n",
    "        optimizer_d.zero_grad()\n",
    "        loss_d_real = criterion(discriminator(real_imgs).view(-1), valid)\n",
    "        noise = torch.randn(N, latent_dim, 1, 1, device=device)\n",
    "        fake_imgs = generator(noise).detach()\n",
    "        loss_d_fake = criterion(discriminator(fake_imgs).view(-1), fake)\n",
    "        loss_d = 0.5 * (loss_d_real + loss_d_fake)\n",
    "        loss_d.backward()\n",
    "        optimizer_d.step()\n",
    "\n",
    "        # --- Gerador ---\n",
    "        optimizer_g.zero_grad()\n",
    "        noise = torch.randn(N, latent_dim, 1, 1, device=device)\n",
    "        gen_imgs = generator(noise)\n",
    "        loss_g = criterion(discriminator(gen_imgs).view(-1), valid)\n",
    "        loss_g.backward()\n",
    "        optimizer_g.step()\n",
    "\n",
    "        d_loss += loss_d.item()\n",
    "        g_loss += loss_g.item()\n",
    "\n",
    "    g_losses.append(g_loss / len(train_dataloader))\n",
    "    d_losses.append(d_loss / len(train_dataloader))\n",
    "\n",
    "    if epoch % plot_interval == 0 or epoch == num_epochs - 1:\n",
    "        print(f\"[{epoch}/{num_epochs-1}] Loss D: {d_losses[-1]:.4f} | Loss G: {g_losses[-1]:.4f}\")\n",
    "        generator.eval()\n",
    "        with torch.no_grad():\n",
    "            noise = torch.randn(8, latent_dim, 1, 1, device=device)\n",
    "            imgs = generator(noise).cpu()\n",
    "        fig, axs = plt.subplots(2, 4, figsize=(8, 4))\n",
    "        for i, ax in enumerate(axs.flat):\n",
    "            ax.imshow(imgs[i].squeeze(), cmap=\"gray\")\n",
    "            ax.axis(\"off\")\n",
    "        plt.suptitle(f'Época {epoch}')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c629a0e",
   "metadata": {},
   "source": [
    "### Análise dos Resultados\n",
    "\n",
    "Após o treinamento, podemos analisar os resultados de duas formas principais: visualizando a curva de perda do Gerador e do Discriminador e observando as imagens geradas ao longo do tempo. Idealmente, as perdas de $D$ e $G$ devem convergir para um estado de equilíbrio, embora na prática elas flutuem bastante. A análise mais importante é a qualidade visual das imagens geradas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d333d2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotar o gráfico de perdas\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.title(\"Generator and Discriminator Loss During Training\")\n",
    "plt.plot(g_losses, label=\"G\")\n",
    "plt.plot(d_losses, label=\"D\")\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d068a5",
   "metadata": {},
   "source": [
    "## Exercícios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770c4e47",
   "metadata": {},
   "source": [
    "### Exercício 1\n",
    "Modifique a dimensão do espaço latente (`latent_dim`). Teste valores como 50 e 200. Adicionalmente, altere a taxa de aprendizado (`lr`) para valores maiores (e.g., 0.002) e menores (e.g., 0.00005). Observe e documente como essas mudanças afetam a velocidade de convergência do treinamento, a estabilidade das perdas do gerador e do discriminador, e a qualidade visual final das imagens geradas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7800cf3c",
   "metadata": {},
   "source": [
    "### Exercício 2\n",
    "Adapte o código para treinar a DCGAN no dataset Fashion-MNIST, que também possui imagens em escala de cinza de 28x28. Como o Fashion-MNIST é visualmente mais complexo que o MNIST, analise se o modelo requer mais épocas para gerar imagens de boa qualidade."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
