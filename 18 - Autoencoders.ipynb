{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f519d73",
   "metadata": {},
   "source": [
    "# Representações Latentes e Autoencoders\n",
    "\n",
    "Neste notebook, exploraremos o conceito de representações latentes e como os autoencoders podem ser utilizados para aprendê-las de forma não supervisionada. Abordaremos os seguintes tópicos:\n",
    "\n",
    "* **Representações Latentes**:\n",
    "    * Definição formal e o objetivo de aprender representações de dados em espaços de menor dimensionalidade.\n",
    "    * Visualização do espaço latente com o algoritmo t-SNE (t-Distributed Stochastic Neighbor Embedding).\n",
    "    * Exemplo prático de treinamento de um codificador (encoder) para a tarefa de classificação e a subsequente visualização de suas representações latentes para o dataset MNIST.\n",
    "\n",
    "* **Autoencoders**:\n",
    "    * Arquitetura fundamental de um autoencoder (encoder-decoder).\n",
    "    * Implementação e treinamento de um autoencoder para reconstruir imagens do MNIST.\n",
    "    * Visualização de imagens originais e reconstruídas.\n",
    "\n",
    "* **Denoising Autoencoders**:\n",
    "    * Conceito e formulação de autoencoders com remoção de ruído.\n",
    "    * Aplicação prática na remoção de ruído de imagens do MNIST."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9aa904",
   "metadata": {},
   "source": [
    "## Representações Latentes\n",
    "\n",
    "No contexto de Deep Learning, uma representação latente é uma codificação interna dos dados de entrada que emerge como resultado do processo de treinamento de uma rede neural para uma tarefa específica. Em vez de ser um resultado de um algoritmo de redução de dimensionalidade pré-definido, o espaço latente é aprendido dinamicamente.\n",
    "\n",
    "As redes neurais profundas são compostas por uma sequência de camadas. Cada camada executa uma transformação sobre seu dado de entrada, e o resultado é uma representação progressivamente mais abstrata. A saída de qualquer camada intermediária de uma rede pode ser considerada uma representação latente.\n",
    "\n",
    "A estrutura e as propriedades deste espaço latente são diretamente influenciadas pela função objetivo (loss function) que o modelo otimiza.\n",
    "\n",
    "Em Aprendizagem Supervisionada (e.g., Classificação), o modelo é treinado para minimizar um erro de classificação (como a Entropia Cruzada). Para isso, o algoritmo de backpropagation ajusta os pesos da rede de forma a transformar os dados de entrada em representações internas que tornem as classes o mais separáveis possível. Um espaço latente ideal, neste caso, agrupará amostras da mesma classe em regiões coesas e distintas, idealmente permitindo uma separação linear por parte das camadas finais da rede. A representação aprende a reter apenas as características discriminativas para a tarefa.\n",
    "\n",
    "Formalmente, o mapeamento para o espaço latente é uma função parametrizada $\\mathbf{z} = f_{\\theta}(x)$, onde $\\theta$ representa os pesos da rede (o *encoder*) e $\\mathbf{z} \\in \\mathbb{R}^m$ é o vetor latente. Esses parâmetros $\\theta$ são aprendidos através da otimização de gradiente descendente para minimizar a função de perda da tarefa final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad7c6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dee7037",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26d453a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Transformação\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# Datasets\n",
    "train_data = datasets.MNIST(\"./data\", train=True, download=True, transform=transform)\n",
    "test_data  = datasets.MNIST(\"./data\", train=False, download=True, transform=transform)\n",
    "\n",
    "# Subsets menores\n",
    "train_subset = Subset(train_data, torch.randperm(len(train_data))[:10000])\n",
    "val_subset   = Subset(test_data,  torch.randperm(len(test_data))[:1000])\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(train_subset, batch_size=64, shuffle=True)\n",
    "val_loader   = DataLoader(val_subset, batch_size=64)\n",
    "\n",
    "print(len(train_subset), len(val_subset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba13185",
   "metadata": {},
   "source": [
    "## Autoencoders\n",
    "\n",
    "Um Autoencoder é um tipo de rede neural artificial utilizada para aprender representações de dados de forma não supervisionada. A arquitetura é composta por duas partes principais:\n",
    "\n",
    "1.  **Encoder ($f$)**: Mapeia a entrada $x$ para uma representação latente $z$ de menor dimensão. $z = f(x)$.\n",
    "2.  **Decoder ($g$)**: Tenta reconstruir a entrada original a partir da representação latente $z$. $\\hat{x} = g(z)$.\n",
    "\n",
    "O objetivo do treinamento é minimizar o erro de reconstrução, que é a diferença entre a entrada original $x$ e a sua reconstrução $\\hat{x}$. Uma função de perda comum para essa tarefa é o Erro Quadrático Médio (Mean Squared Error - MSE).\n",
    "\n",
    "$$ \\mathcal{L}(x, \\hat{x}) = \\mathcal{L}(x, g(f(x))) = \\frac{1}{N} \\sum_{i=1}^{N} (x_i - \\hat{x}_i)^2 $$\n",
    "\n",
    "O gargalo informacional imposto pela camada latente força o autoencoder a aprender apenas as variações mais importantes nos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8020f603",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, encoding_dim=16):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(28 * 28, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, encoding_dim),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(encoding_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 28 * 28),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e376e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 16\n",
    "ae_model = Autoencoder(encoding_dim=latent_dim).to(device)\n",
    "print(ae_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e1396e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_criterion = nn.MSELoss()\n",
    "ae_optimizer = torch.optim.Adam(ae_model.parameters(), lr=1e-3, weight_decay=1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f0bb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "ae_model.train()\n",
    "\n",
    "train_losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0.0\n",
    "    for X, _ in train_loader:\n",
    "        X = X.view(X.size(0), -1).to(device)\n",
    "        \n",
    "        # Forward\n",
    "        recon = ae_model(X)\n",
    "        loss = ae_criterion(recon, X)\n",
    "\n",
    "        # Backward\n",
    "        ae_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        ae_optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    train_losses.append(avg_loss)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(range(1, epochs+1), train_losses, marker=\"o\")\n",
    "plt.title(\"Training Loss (Autoencoder)\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08288b80",
   "metadata": {},
   "source": [
    "### Visualização das Reconstruções\n",
    "\n",
    "Após o treinamento, podemos passar imagens do conjunto de teste pelo autoencoder para obter suas reconstruções. Comparar visualmente as imagens originais com as reconstruídas nos dá uma avaliação qualitativa do desempenho do modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a857b27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualização das imagens reconstruídas\n",
    "ae_model.eval()\n",
    "n = 10\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Pega um batch de dados de validação\n",
    "    data_iter = iter(val_loader)\n",
    "    images, _ = next(data_iter)\n",
    "    images_flat = images.view(images.size(0), -1).to(device)\n",
    "    \n",
    "    # Gera as reconstruções\n",
    "    reconstructed_flat = ae_model(images_flat)\n",
    "    \n",
    "    # Converte para numpy para plotar\n",
    "    original_images = images.cpu().numpy()\n",
    "    reconstructed_images = reconstructed_flat.view(-1, 1, 28, 28).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e822f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 4))\n",
    "\n",
    "for i in range(n):\n",
    "    # Imagem original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(original_images[i].squeeze(), cmap='gray', vmin=0, vmax=1)\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    if i == n//2:\n",
    "        ax.set_title('Imagens Originais')\n",
    "\n",
    "    # Imagem reconstruída\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(reconstructed_images[i].squeeze(), cmap='gray', vmin=0, vmax=1)\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    if i == n//2:\n",
    "        ax.set_title('Imagens Reconstruídas')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203df1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_model.eval()\n",
    "ae_all_latents = []\n",
    "ae_all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X, y in val_loader:\n",
    "        X = X.view(X.size(0), -1).to(device)\n",
    "        latent = ae_model.encoder(X)\n",
    "        ae_all_latents.append(latent.cpu().numpy())\n",
    "        ae_all_labels.append(y.cpu().numpy())\n",
    "\n",
    "ae_latent_space_test = np.concatenate(ae_all_latents, axis=0)\n",
    "ae_labels_test = np.concatenate(ae_all_labels, axis=0)\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42, verbose=1, perplexity=30, n_iter=1000)\n",
    "tsne_results = tsne.fit_transform(ae_latent_space_test)\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "scatter = plt.scatter(tsne_results[:, 0], tsne_results[:, 1], c=ae_labels_test, cmap='tab10', s=10)\n",
    "plt.title('Visualização do Espaço Latente com t-SNE', fontsize=16)\n",
    "plt.xlabel('Componente t-SNE 1')\n",
    "plt.ylabel('Componente t-SNE 2')\n",
    "plt.legend(handles=scatter.legend_elements()[0], labels=list(range(10)))\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b13e9e6",
   "metadata": {},
   "source": [
    "## Interpolando no espaço latente\n",
    "\n",
    "A partir das representações latentes aprendidas pelo encoder, podemos explorar o espaço latente realizando **interpolação linear** entre dois vetores $z_1$ e $z_2$. Essa técnica serve para verificar se o autoencoder aprendeu um espaço contínuo e semanticamente estruturado, no qual pequenas mudanças em $z$ resultam em mudanças suaves nas reconstruções.\n",
    "\n",
    "Dada uma interpolação linear:\n",
    "\n",
    "$$\n",
    "z(t) = (1 - t) z_1 + t z_2,\\quad t \\in [0, 1],\n",
    "$$\n",
    "\n",
    "podemos decodificar cada ponto intermediário e visualizar a transição entre duas imagens reais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff135ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecionar duas imagens do conjunto de validação\n",
    "ae_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    imgs, _ = next(iter(val_loader))\n",
    "    x1 = imgs[0].view(1, -1).to(device)\n",
    "    x2 = imgs[1].view(1, -1).to(device)\n",
    "\n",
    "    z1 = ae_model.encoder(x1)\n",
    "    z2 = ae_model.encoder(x2)\n",
    "\n",
    "# Interpolar entre z1 e z2\n",
    "steps = 12\n",
    "interpolations = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for t in np.linspace(0, 1, steps):\n",
    "        zt = (1 - t) * z1 + t * z2\n",
    "        xt = ae_model.decoder(zt).view(28, 28).cpu().numpy()\n",
    "        interpolations.append(xt)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(20, 3))\n",
    "for i, img in enumerate(interpolations):\n",
    "    ax = plt.subplot(1, steps, i + 1)\n",
    "    plt.imshow(img, cmap=\"gray\", vmin=0, vmax=1)\n",
    "    ax.axis(\"off\")\n",
    "plt.suptitle(\"Interpolação Linear no Espaço Latente\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8cfbd10",
   "metadata": {},
   "source": [
    "## Gerando novos exemplos\n",
    "\n",
    "Após o treinamento, o decoder pode ser usado isoladamente para gerar novas imagens a partir de vetores latentes amostrados manualmente. Como o espaço latente do autoencoder não é regularizado, essas amostras não seguem uma distribuição específica; ainda assim, podemos testar valores aleatórios em $\\mathbb{R}^m$ e observar as reconstruções produzidas.\n",
    "\n",
    "O procedimento é simples:\n",
    "1. Amostre um vetor aleatório $z$ (por exemplo, de uma distribuição normal padrão).\n",
    "2. Passe $z$ pelo decoder.\n",
    "3. Visualize o resultado como uma imagem de 28×28 pixels.\n",
    "\n",
    "Isso nos permite avaliar o quanto o modelo aprendeu a estrutura dos dígitos no espaço latente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ac7526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerar novas imagens a partir de vetores latentes aleatórios\n",
    "ae_model.eval()\n",
    "\n",
    "num_samples = 12\n",
    "rand_z = torch.randn(num_samples, latent_dim).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    generated = ae_model.decoder(rand_z).view(-1, 28, 28).cpu().numpy()\n",
    "\n",
    "plt.figure(figsize=(20, 3))\n",
    "for i in range(num_samples):\n",
    "    ax = plt.subplot(1, num_samples, i + 1)\n",
    "    plt.imshow(generated[i], cmap='gray', vmin=0, vmax=1)\n",
    "    ax.axis(\"off\")\n",
    "plt.suptitle(\"Imagens Geradas a Partir de Amostras Aleatórias do Latent Space\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588f45ca",
   "metadata": {},
   "source": [
    "### Por quê a geração é problemática?\n",
    "\n",
    "Autoencoders tradicionais não impõem nenhuma estrutura probabilística ao espaço latente. Isso cria vários problemas:\n",
    "\n",
    "* O espaço latente não segue uma distribuição simples (por exemplo, uma normal isotrópica).  \n",
    "* Regiões do espaço latente podem não corresponder a imagens válidas.  \n",
    "* Amostrar $z$ aleatoriamente leva o decoder a áreas que ele nunca viu durante o treinamento.  \n",
    "* O modelo não aprende um mapeamento explícito entre uma distribuição simples e a distribuição dos dados.\n",
    "\n",
    "Como consequência, a geração tende a produzir imagens distorcidas ou incoerentes. Esse é exatamente o ponto em que entram os **Variational Autoencoders (VAEs)**: eles forçam o espaço latente a seguir uma distribuição bem definida, permitindo geração consistente e controlada."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
